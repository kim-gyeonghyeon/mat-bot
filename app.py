import streamlit as st
import os
import google.generativeai as genai

# --- [ì„¤ì •] ---
# ë°©ê¸ˆ ìƒˆë¡œ ë°œê¸‰ë°›ìœ¼ì‹  API í‚¤ë¥¼ ì—¬ê¸°ì— ì •í™•íˆ ì…ë ¥í•˜ì„¸ìš”.
GOOGLE_API_KEY = "AIzaSyCdyr7CbuHNIff8PWYWRNwcw4hSVf6FWok"
genai.configure(api_key=GOOGLE_API_KEY)
DATA_FILE = "rules.txt"
# --------------

st.set_page_config(page_title="ì‚¬ë‚´ê·œì • ì±—ë´‡", page_icon="ğŸ¤–")
st.title("ğŸ“‚ ì— ì—ì´í‹°í”ŒëŸ¬ìŠ¤ ì‚¬ë‚´ê·œì • ì±—ë´‡")

# íŒŒì¼ ë¡œë“œ í•¨ìˆ˜ (Streamlit Cloud ê²½ë¡œ ìµœì í™”)
def get_rules():
    path = os.path.join(os.path.dirname(os.path.abspath(__file__)), DATA_FILE)
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    return None

rules_text = get_rules()

# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ (404 ì—ëŸ¬ ë°©ì§€ìš© 3ë‹¨ê³„ ë¡œì§)
@st.cache_resource
def load_validated_model():
    # ì‹œë„í•  ëª¨ë¸ëª… ë¦¬ìŠ¤íŠ¸ (êµ¬ê¸€ APIê°€ ì¸ì‹í•˜ëŠ” í‘œì¤€ ëª…ì¹­ë“¤)
    model_candidates = [
        'models/gemini-1.5-flash-latest', 
        'models/gemini-1.5-flash', 
        'models/gemini-pro'
    ]
    
    for name in model_candidates:
        try:
            model = genai.GenerativeModel(name)
            # ì‹¤ì œë¡œ ëŒ€ë‹µì´ ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸ (ì—¬ê¸°ì„œ ì—ëŸ¬ë‚˜ë©´ ë‹¤ìŒ ëª¨ë¸ë¡œ)
            model.generate_content("ping") 
            return model
        except:
            continue
    return None

if rules_text:
    model = load_validated_model()
    
    if model:
        st.success("âœ… ê·œì • í™•ì¸ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        if user_input := st.chat_input("ê·œì •ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”"):
            st.session_state.messages.append({"role": "user", "content": user_input})
            with st.chat_message("user"):
                st.markdown(user_input)

            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    prompt = f"ë‹¤ìŒ ê·œì •ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´:\n{rules_text}\n\nì§ˆë¬¸: {user_input}"
                    try:
                        response = model.generate_content(prompt)
                        st.markdown(response.text)
                        st.session_state.messages.append({"role": "assistant", "content": response.text})
                    except Exception as e:
                        st.error(f"AI ì‘ë‹µ ì—ëŸ¬: {e}")
    else:
        st.error("âŒ êµ¬ê¸€ AI ëª¨ë¸ ì—°ê²°ì— ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í‚¤ì˜ ìœ íš¨ì„±ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    st.error(f"'{DATA_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GitHubì— íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
