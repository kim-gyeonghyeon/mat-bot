import streamlit as st
import os
import google.generativeai as genai

# --- [ì„¤ì •] ---
# ë§Œì•½ ì—ëŸ¬ê°€ ê³„ì†ëœë‹¤ë©´, ì—¬ê¸°ì„œ ìƒˆë¡œìš´ API í‚¤ë¥¼ ë°œê¸‰ë°›ì•„ êµì²´í•˜ì„¸ìš”.
GOOGLE_API_KEY = "AIzaSyCdyr7CbuHNIff8PWYWRNwcw4hSVf6FWok"
genai.configure(api_key=GOOGLE_API_KEY)
DATA_FILE = "rules.txt"
# --------------

st.set_page_config(page_title="ì‚¬ë‚´ê·œì • ì±—ë´‡", page_icon="ğŸ¤–")
st.title("ğŸ“‚ ì— ì—ì´í‹°í”ŒëŸ¬ìŠ¤ ì‚¬ë‚´ê·œì • ì±—ë´‡")

# 1. íŒŒì¼ ì½ê¸° í•¨ìˆ˜
def get_rules():
    # Streamlit Cloud í™˜ê²½ì—ì„œë„ íŒŒì¼ì„ ì •í™•íˆ ì°¾ë„ë¡ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    path = os.path.join(os.path.dirname(os.path.abspath(__file__)), DATA_FILE)
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    return None

rules_text = get_rules()

# 2. ëª¨ë¸ ì„¤ì • (ê°€ì¥ ì•ˆì •ì ì¸ gemini-pro ì‚¬ìš©)
@st.cache_resource
def load_model():
try:
    # ê°€ì¥ í‘œì¤€ì ì´ê³  íŠ¼íŠ¼í•œ ëª¨ë¸ëª…ì…ë‹ˆë‹¤.
    model = genai.GenerativeModel('models/gemini-1.5-flash-latest')
    # ì—°ê²° í…ŒìŠ¤íŠ¸
    model.generate_content("test")
except:
    try:
        # ë‘ ë²ˆì§¸ ëŒ€ì•ˆ
        model = genai.GenerativeModel('models/gemini-1.5-pro-latest')
    except Exception as e:
        st.error(f"ëª¨ë¸ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (ìƒì„¸ì—ëŸ¬: {e})")
        return None

if rules_text:
    model = load_model()
    
    if model:
        st.success("âœ… ê·œì • í™•ì¸ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    
        # ëŒ€í™” ì„¸ì…˜ ì´ˆê¸°í™”
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # ê¸°ì¡´ ëŒ€í™” í‘œì‹œ
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        # ì‚¬ìš©ì ì…ë ¥
        if user_input := st.chat_input("ê·œì •ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”"):
            st.session_state.messages.append({"role": "user", "content": user_input})
            with st.chat_message("user"):
                st.markdown(user_input)

            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    # ê·œì • ì „ë¬¸ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì§ì ‘ ë„£ì–´ ì§ˆë¬¸í•©ë‹ˆë‹¤.
                    prompt = f"ë‹¤ìŒ ì‚¬ë‚´ ê·œì •ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n\n[ê·œì • ë‚´ìš©]\n{rules_text}\n\nì§ˆë¬¸: {user_input}"
                    try:
                        response = model.generate_content(prompt)
                        ans = response.text
                        st.markdown(ans)
                        st.session_state.messages.append({"role": "assistant", "content": ans})
                    except Exception as e:
                        st.error(f"AI ì‘ë‹µ ì—ëŸ¬: {e}")
                        st.info("API í‚¤ê°€ ë§Œë£Œë˜ì—ˆê±°ë‚˜ ëª¨ë¸ ê¶Œí•œì´ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ í‚¤ë¥¼ ë°œê¸‰ë°›ì•„ë³´ì„¸ìš”.")
else:
    st.error(f"'{DATA_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.info("GitHubì— rules.txt íŒŒì¼ì´ app.pyì™€ ê°™ì€ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

